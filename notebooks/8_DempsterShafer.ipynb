{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sejRzX5WaYiW",
        "outputId": "b708ede2-b436-4549-b285-ddfeeaa9b3d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Combined Model Performance:\n",
            "Accuracy: 0.9769\n",
            "Precision: 0.9629\n",
            "Recall: 0.9440\n",
            "F1-Score: 0.9533\n",
            "AUC-ROC: 0.9961\n",
            "\n",
            "Model Weights:\n",
            "LSTM: 0.3321\n",
            "GRU: 0.3334\n",
            "CNN: 0.3345\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Connect to Google Drive and load models\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load models\n",
        "model_lstm = tf.keras.models.load_model('/content/drive/MyDrive/Thesis/backorder_lstm_model_dataset_3_2.h5')\n",
        "model_gru = tf.keras.models.load_model('/content/drive/MyDrive/Thesis/backorder_gru_model_dataset_3.h5')\n",
        "model_cnn = tf.keras.models.load_model('/content/drive/MyDrive/Thesis/backorder_cnn_model_dataset_3.h5')\n",
        "\n",
        "# 2. Prepare test data\n",
        "def create_sequences(data, seq_length=5):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        seq = data.iloc[i:i+seq_length, :-1].values\n",
        "        label = data.iloc[i+seq_length, -1]\n",
        "        sequences.append(seq)\n",
        "        targets.append(label)\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "SEQ_LENGTH = 5\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Thesis/Test_dataset_3.csv')\n",
        "X_test, y_test = create_sequences(test_data, SEQ_LENGTH)\n",
        "\n",
        "# 3. Get predictions from models\n",
        "def get_predictions(model, X):\n",
        "    y_pred = (model.predict(X) > 0.5).astype(int).flatten()\n",
        "    y_proba = model.predict(X).flatten()\n",
        "    return y_pred, np.column_stack([1-y_proba, y_proba])\n",
        "\n",
        "lstm_pred, lstm_proba = get_predictions(model_lstm, X_test)\n",
        "gru_pred, gru_proba = get_predictions(model_gru, X_test)\n",
        "cnn_pred, cnn_proba = get_predictions(model_cnn, X_test)\n",
        "\n",
        "# 4. Calculate model metrics\n",
        "def calculate_metrics(y_true, y_pred, y_proba):\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred),\n",
        "        'recall': recall_score(y_true, y_pred),\n",
        "        'f1': f1_score(y_true, y_pred),\n",
        "        'auc': roc_auc_score(y_true, y_proba[:, 1]),\n",
        "        'error_rate': 1 - accuracy_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "models_results = {\n",
        "    'LSTM': {\n",
        "        'predictions': lstm_pred,\n",
        "        'probabilities': lstm_proba,\n",
        "        **calculate_metrics(y_test, lstm_pred, lstm_proba)\n",
        "    },\n",
        "    'GRU': {\n",
        "        'predictions': gru_pred,\n",
        "        'probabilities': gru_proba,\n",
        "        **calculate_metrics(y_test, gru_pred, gru_proba)\n",
        "    },\n",
        "    'CNN': {\n",
        "        'predictions': cnn_pred,\n",
        "        'probabilities': cnn_proba,\n",
        "        **calculate_metrics(y_test, cnn_pred, cnn_proba)\n",
        "    }\n",
        "}\n",
        "\n",
        "# 5. Fixed Dempster-Shafer Fusion Class\n",
        "class DempsterShaferFusion:\n",
        "    def __init__(self, models_results):\n",
        "        self.models_results = models_results\n",
        "        self.models = list(models_results.keys())\n",
        "\n",
        "    def calculate_model_weights(self):\n",
        "        \"\"\"Calculate weights based on model performance\"\"\"\n",
        "        weights = {}\n",
        "        for model in self.models:\n",
        "            metrics = self.models_results[model]\n",
        "            score = 0.4*metrics['auc'] + 0.3*metrics['f1'] + 0.2*metrics['accuracy'] + 0.1*metrics['precision']\n",
        "            weights[model] = max(score, 0.1)  # Ensure minimum weight\n",
        "\n",
        "        # Normalize weights\n",
        "        total = sum(weights.values())\n",
        "        return {model: weight/total for model, weight in weights.items()}\n",
        "\n",
        "    def calculate_bpa(self):\n",
        "        \"\"\"Calculate Basic Probability Assignment (Fixed Version)\"\"\"\n",
        "        bpa = {}\n",
        "        weights = self.calculate_model_weights()\n",
        "        n_samples = len(self.models_results[self.models[0]]['probabilities'])\n",
        "\n",
        "        for model in self.models:\n",
        "            probs = self.models_results[model]['probabilities']\n",
        "            weight = weights[model]\n",
        "            error_rate = self.models_results[model]['error_rate']\n",
        "\n",
        "            # Initialize arrays\n",
        "            believe_0 = np.zeros(n_samples)\n",
        "            believe_1 = np.zeros(n_samples)\n",
        "            uncertainty = np.zeros(n_samples)\n",
        "\n",
        "            for i in range(n_samples):\n",
        "                # Calculate belief masses\n",
        "                believe_0[i] = (1 - probs[i, 1]) * (1 - error_rate) * weight\n",
        "                believe_1[i] = probs[i, 1] * (1 - error_rate) * weight\n",
        "                uncertainty[i] = error_rate * weight\n",
        "\n",
        "                # Normalize\n",
        "                total = believe_0[i] + believe_1[i] + uncertainty[i]\n",
        "                if total > 0:\n",
        "                    believe_0[i] /= total\n",
        "                    believe_1[i] /= total\n",
        "                    uncertainty[i] /= total\n",
        "\n",
        "            bpa[model] = {\n",
        "                'believe_0': believe_0,\n",
        "                'believe_1': believe_1,\n",
        "                'uncertainty': uncertainty\n",
        "            }\n",
        "\n",
        "        return bpa\n",
        "\n",
        "    def combine_evidence(self, bpa):\n",
        "        \"\"\"Combine evidence using Dempster's rule\"\"\"\n",
        "        n_samples = len(bpa[self.models[0]]['believe_0'])\n",
        "        combined_0 = np.zeros(n_samples)\n",
        "        combined_1 = np.zeros(n_samples)\n",
        "        combined_uncertainty = np.zeros(n_samples)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            # Initialize with first model's BPA\n",
        "            current_0 = bpa[self.models[0]]['believe_0'][i]\n",
        "            current_1 = bpa[self.models[0]]['believe_1'][i]\n",
        "            current_u = bpa[self.models[0]]['uncertainty'][i]\n",
        "\n",
        "            # Combine with other models\n",
        "            for model in self.models[1:]:\n",
        "                m_0 = bpa[model]['believe_0'][i]\n",
        "                m_1 = bpa[model]['believe_1'][i]\n",
        "                m_u = bpa[model]['uncertainty'][i]\n",
        "\n",
        "                # Calculate conflict\n",
        "                K = current_0*m_1 + current_1*m_0\n",
        "\n",
        "                if K < 1:  # Only combine if conflict is not total\n",
        "                    # Combine beliefs\n",
        "                    new_0 = (current_0*m_0 + current_0*m_u + current_u*m_0) / (1 - K)\n",
        "                    new_1 = (current_1*m_1 + current_1*m_u + current_u*m_1) / (1 - K)\n",
        "                    new_u = (current_u * m_u) / (1 - K)\n",
        "\n",
        "                    # Normalize\n",
        "                    total = new_0 + new_1 + new_u\n",
        "                    if total > 0:\n",
        "                        current_0, current_1, current_u = new_0/total, new_1/total, new_u/total\n",
        "\n",
        "            combined_0[i] = current_0\n",
        "            combined_1[i] = current_1\n",
        "            combined_uncertainty[i] = current_u\n",
        "\n",
        "        return combined_0, combined_1, combined_uncertainty\n",
        "\n",
        "    def predict(self):\n",
        "        \"\"\"Make final predictions\"\"\"\n",
        "        bpa = self.calculate_bpa()\n",
        "        belief_0, belief_1, uncertainty = self.combine_evidence(bpa)\n",
        "\n",
        "        predictions = (belief_1 > belief_0).astype(int)\n",
        "        combined_probs = np.column_stack((belief_0, belief_1))\n",
        "\n",
        "        return predictions, combined_probs, uncertainty\n",
        "\n",
        "# 6. Apply Dempster-Shafer Fusion\n",
        "ds_fusion = DempsterShaferFusion(models_results)\n",
        "final_pred, final_proba, uncertainty = ds_fusion.predict()\n",
        "\n",
        "# 7. Evaluate and save results\n",
        "print(\"\\nCombined Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, final_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, final_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, final_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, final_pred):.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_test, final_proba[:, 1]):.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'True_Label': y_test,\n",
        "    'Final_Prediction': final_pred,\n",
        "    'Probability_Class_0': final_proba[:, 0],\n",
        "    'Probability_Class_1': final_proba[:, 1],\n",
        "    'Uncertainty': uncertainty\n",
        "})\n",
        "\n",
        "# Add individual model results\n",
        "for model in models_results:\n",
        "    results_df[f'{model}_Pred'] = models_results[model]['predictions']\n",
        "    results_df[f'{model}_Prob0'] = models_results[model]['probabilities'][:, 0]\n",
        "    results_df[f'{model}_Prob1'] = models_results[model]['probabilities'][:, 1]\n",
        "\n",
        "results_df.to_csv('/content/drive/MyDrive/Thesis/dempster_shafer_fixed_results.csv', index=False)\n",
        "\n",
        "# Show model weights\n",
        "weights = ds_fusion.calculate_model_weights()\n",
        "print(\"\\nModel Weights:\")\n",
        "for model, weight in weights.items():\n",
        "    print(f\"{model}: {weight:.4f}\")"
      ]
    }
  ]
}